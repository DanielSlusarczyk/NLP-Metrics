{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fca8cd8-67ea-49ea-ad53-f0f0676fcf9f",
   "metadata": {},
   "source": [
    "## Projekt Indywidualny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed205819-c5d2-4a22-a126-1196cc7fab40",
   "metadata": {},
   "source": [
    "Autor: <b>Daniel Ślusarczyk</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b5c04-b05b-49ef-829c-6f4f2f91cbc5",
   "metadata": {},
   "source": [
    "Opiekun projektu: <b>mgr inż. Mateusz Bartosiewicz<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186a5bd-49ca-4bdd-974d-2eb6b29e2716",
   "metadata": {},
   "source": [
    "## Spis Treści:\n",
    "1. [Cel projektu](#CP)\n",
    "2. [NLP](#NLP)\n",
    "3. [Potrzebne instalacje](#PI)\n",
    "4. [Metryki](#M)\n",
    "    1. [BLEU](#BLEU)\n",
    "        1. [Opis](#BLEU_O)\n",
    "        2. [Działanie](#BLEU_D)\n",
    "        3. [Problemy](#BLEU_P)\n",
    "    2. [METOR](#METEOR)\n",
    "        1. [Opis](#METEOR_O)\n",
    "        2. [Działanie](#METEOR_D)\n",
    "    3. [ROUGE](#ROUGE)\n",
    "        1. [Opis](#ROUGE_O)\n",
    "        2. [ROUGE-N](#ROUGE_N)\n",
    "            1. [Działanie](#ROUGE_N_D)\n",
    "        3. [ROUGE-L](#ROUGE_L)\n",
    "            1. [Działanie](#ROUGE_L_D)\n",
    "    4. [WMD](#WMD)\n",
    "        1. [Opis](#WMD_O)\n",
    "        2. [Działanie](#WMD_D)\n",
    "        3. [Problemy](#WMD_P)\n",
    "5. [Sposoby porównywania tesktów](#SPT)\n",
    "6. [Embeddings dla NLP](#EDN)\n",
    "7. [Zbiór COCO](#ZC)\n",
    "8. [Serwer ewauacyjny COCO](#SEC)\n",
    "9. [Źródła](#Z)\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0b6d8-96e1-4fd4-93f1-bf61331cb411",
   "metadata": {},
   "source": [
    "### Cel projektu\n",
    "<a name=\"CP\"></a>\n",
    "Celem projektu jest opracowanie teoretyczne narzędzi do analizy NLP.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b7806-43a0-4b6b-97dc-76e2e55a50e8",
   "metadata": {},
   "source": [
    "### Potrzebne instalacje\n",
    "<a name=\"PI\"></a>\n",
    "<p style='text-align: justify;'>\n",
    "W celu wizualizacji sposobu działania niektórych metryk niezbędna jest instalacja dodatkowych pakietów, które nie są dostępne w standardowym Pythonie. Niezainstalowanie tych pakietów wiąże się z niewłaściwym działaniem kodu znajdującego się w poniższych punktach.\n",
    "</p>\n",
    "\n",
    "Niezbędne instalacje:\n",
    "* Metryka ROUGE - Python rouge library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e33ee-4841-488f-a72a-61fab7d443a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pakiety do liczenia wzorcowego ROUGE\n",
    "import sys\n",
    "!{sys.executable} -m pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a158b5f-c1a5-4306-8c39-7417ebb1b1f8",
   "metadata": {},
   "source": [
    "* Metryka WMD - NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0095eb07-a9c0-4c0e-9fe8-582c0feb3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pakiety do usuwania \"przerywników\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b603bb-d2fc-4dff-925b-407e10fce919",
   "metadata": {},
   "source": [
    "* Metryka WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4644ec-c6c9-4853-aa0c-87a3b18a1451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pyemd is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file).\n"
     ]
    }
   ],
   "source": [
    "# Pakiety do liczenia dystansu WMD\n",
    "import sys\n",
    "!pip install -e pyemd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1520a-1777-400a-96e9-98349ba54343",
   "metadata": {},
   "source": [
    "### NLP - Przetwarzanie Języka Naturalnego\n",
    "<a id =\"NLP\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633653e0-936b-4481-b606-70cc3885a22f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style='text-align: justify;'>\n",
    "NLP (z ang. Natural Leanguage Processing) jest to interdyscyplinarna dziedzina, oparta na podstawach sztucznej inteligencji i językoznawstwa.\n",
    "Zajmuje się automatyzacją analizy, rozumienia, tłumaczenia i generowania języka naturalnego przez komputer.  Istnieją dwa fundamentalne kierunki przepływu informacji w NLP, które stanowią główną problematykę tej dziedziny . System, który zawiera informacje zapisane w bazie danych w sposób techniczny i zrozumiały wyłącznie dla osób zaznajomionych z sposobem zapisu przekształca się w informacje przedstawione w sposób zrozumiały dla wszystkich osób posługujących się danym językiem. Zaś system, który rozumie język naturalny modyfikuje go na formalne symbole możliwe do przetworzenia przez system komputerowy. W konsekwencji problematyka NLP dotyczy zarówno generacji i rozumienia języka.\n",
    "</p>\n",
    "<br>\n",
    "<b>Problemy stojące przed NLP:</b> \n",
    "<br>\n",
    "Ze względu na niezwykłe rozbudowanie i skomplikowanie języka naturalnego można wyróżnić wiele problemów, z którymi wiąże się dziedzina NLP:\n",
    "<ul>\n",
    "<li> Segmentacja sygnału mowy</li>\n",
    "<li> Segmentacja tekstu</li>\n",
    "<li> Wieloznaczność słów</li>\n",
    "<li> Syntaktyczna niejednoznaczność</li>\n",
    "<li> Nieprawidłowe, bądź nieregularne dane</li>\n",
    "<li> Akt mowy i plan</li>\n",
    "</ul>\n",
    "<b> Przykładowe zadania NLP:</b>\n",
    "<ul>\n",
    "<li> Automatyczna sumaryzacja – program umożliwiający streszczenie dłuższego tekstu w krótszy o tym samym przesłaniu i najważniejszych informacjach</li>\n",
    "<li> Synteza mowy – operacja polegająca na przetwarzaniu języka na mowę</li>\n",
    "<li> Korekcja tekstu – analiza tekstu i wykrywanie błędów</li>\n",
    "<li> Rozpoznawanie mowy – operacja polegająca na przetwarzaniu mowy na tekst </li>\n",
    "</ul>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006d249-29de-44e7-bd07-b5f404a225e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metryki \n",
    "<a class=\"anchor\" id =\"M\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea2d274-6a4b-4e26-8c92-a05d7e0af6e0",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Wzmożony rozwój NLP skutkuje zwiększoną potrzebą oceny jakosci powstających systemów automatycznego przetwarzania języka naturalnego. Metryki są nieodłączonym elementem uczenia maszynowego. Służa do oceny spełnienia oczekiwań stawianych przed rozwiązaniem problememu, do którego używany jest ewaluowany system.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8d204-3929-44ac-a886-7856f8ab5975",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "<a class=\"anchor\" id =\"BLEU\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51175e7d-4bba-4167-a56b-0d8d7c6368e4",
   "metadata": {},
   "source": [
    "<b><font color='red'>B</font>i<font color='red'>L</font>ingual <font color='red'>E</font>valuation <font color='red'>U</font>nderstudy</b>\n",
    "<a class=\"anchor\" id =\"BLEU_O\"></a>\n",
    "<br>\n",
    "<p style='text-align: justify;'>\n",
    "Ewaluacja służąca do mierzenia jakości modeli tłumaczenia maszynowego kierująca się zasadą \"im bliższe tłumaczenie automatyczne i profesjonalne ludzkie tłumaczenie, tym lepiej\". Zadaniem tej metryki jest ocena jak dobrze model tłumaczy tekst pomiędzy językami. W przypadku tej metryki \"jakość\" rozumiana jest jako korelacja pomiędzy danymi wyjściowymi a tekstem ludzkim. Został przedstawiony w 2002 roku i opisany w raporcie firmy IBM. BLEU jest jedną z pierwszych metryk, której udało się uzyskać wyniki zbliżone z ludzkim osądem. W konsekwencji stała się najbardziej popularną metodą, pomimo pewnych wad.\n",
    "</p>\n",
    "\n",
    "<b>Działanie</b><br>\n",
    "<a id =\"BLEU_D\"></a>\n",
    "<p style='text-align: justify;'>\n",
    "Działanie metryki zostanie przedstawione na podstawie kodu, który krok po kroku przeprowadza potrzebne operacje w celu obliczenia wartości metryki. Prezentowany kod ma charakter prezentacyjny i wizualizuje wiele informacji w celu łatwiejszego zrozumienia działania metryki. Wynik uzyskany przez kod jest możliwy do porównania ze wzorcowym wynikiem.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d791e-962b-4e0f-a5ea-fc018e3cf720",
   "metadata": {},
   "source": [
    "Import potrzebnych pakietów: <br>\n",
    "<p style='text-align: justify;'>\n",
    "Pakiet \"nltk.translate.bleu_score\" importowany jako alias \"bleu\" służy do uzyskania wzorcowego wyniku na końcu przykładu w celu porównania go z wynikiem uzyskanym przez\n",
    "kod.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eef6121-c0e2-4a3a-ab0b-45a05740add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wzorcowa wartość BLEU:\n",
    "import nltk.translate.bleu_score as bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca0f31-21d5-4a04-ad7e-4dc908999ca0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Funkcje pomocnicze: <br>\n",
    "Kod korzysta z poniższych zdefiniowanych funkcji, które pozwalają na łatwiejszego zrozumienie kodu. Zrozumienie ich działania nie jest konieczne do zrozumienia działania metryki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f06d16e-337d-48be-9e36-1bcf9be7e458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from BLEU_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f755473-d072-4e18-8e21-5cc74980f361",
   "metadata": {},
   "source": [
    "Analizowane dane: <br>\n",
    "<p style='text-align: justify;'>\n",
    "Metryka BLEU opiera działanie na porównywaniu n-gramów tłumaczenia kandydata z n-gramami tłumaczeń wzorcowych (miejsce wystąpienia analizowanego n-gramu w tłumaczeniu kandydatującym i wzorcu nie ma znaczenia). Im większa liczba wspólnych n-gramów pomiędzy kandydatem i wzorcem tym tłumaczenie jest uznawane za lepsze. Metryka BLEU pozwala na dostosowanie wag poszczególnych n-gramów i przypisanie większej wartości n-gramom, które powinny być szczególnie uwzględniane. Wartość listy <b>weights</b> służy do podania wag dla poszczególnych n-gramów. Podanie wartości 0 jest jednoznaczne z wyzerowaniem wpływu danego n-gramu na wynik końcowy. Niepodanie żadnej wartości pozwala całkowicie wykluczyć ostatnie n-gramy z analizy. Lista <b>references</b> służy do podania wzorcowych tłumaczeń dla danego tłumaczenia kandydującego <b>candidate</b>. Sposób prawidłowego wpisania tych danych jest przedstawiony poniżej.<br>\n",
    "</p>\n",
    "Przykłady: <br>\n",
    "weights = (0.25) - uwzględnienie w obliczeniach jedynie unigramów <br>\n",
    "weights = (0, 0, 0.25) - uwzględnienie w wyniku jedynie trigramów. Przeprowadzenie analizy dla unigramów, bigramów i trigramów <br>\n",
    "weights = (0.2, 0.3, 0.4, 0.5) - uwzględnienie w obliczeniach od unigramu do 4-gramu z przypisaniem kolejno wag: 0.2, 0.3, 0.4, 0.5 <br>\n",
    "references = ['Recepcjonista poinformował o zamknięciu hotelu'.split(), 'Pracownik recepcji poinformował o zamknięciu hotelu'.split()] - zdefiniowanie dwóch tłumaczeń wzorcowych dla tłumaczenia kandydującego <br>\n",
    "references = ['Recepcjonista poinformował o zamknięciu hotelu'.split()] - zdefiniowanie jednego tłumaczenia wzorcowego dla tłumaczenia kandydującego <br>\n",
    "candidate = 'Pan pracujący na recepecji udzielił informacji o zamknięciu hotelu'.split() - zdefiniowanie ocenianego tłumaczenia (kandydata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b586094b-c7d7-4602-8ef6-8429fdc2e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wagi przypisane do poszczególnych n-gramów:\n",
    "weights = [0.25, 0.25, 0.25]\n",
    "\n",
    "# Tłumaczenia referencyjne\n",
    "references = [\n",
    "            'It is guide to action that ensures that the miliatry will forever heed Party commands', \n",
    "            'It is the guide principle which guarantees the miliatry forces always being under the command of the Party',\n",
    "            'It is the practical guide for the army always to heed the directions of the party'\n",
    "            ]\n",
    "references = splitList(references)\n",
    "\n",
    "# Tłumaczenie kandydujące\n",
    "candidate = 'It is a guide to action which ensures that the miliatry always obeys the commands of the party'\n",
    "candidate = candidate.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190f9c5-60d4-424f-a2e8-401a38720ebd",
   "metadata": {},
   "source": [
    "Potwierdzenie danych: <br>\n",
    "Wykonanie poniższego kodu pozwala na potwierdzenie poprawności wprowadzonych danych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3039a319-5307-46c1-ae14-c6ac32c928ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba rozpatrywanych n-gramów:                   3\n",
      "Liczba tłumaczeń referencyjnych (wzorcowych):     3\n",
      "Maksymalna długość wzorca:                        18\n",
      "Maksymalna długość kandydata:                     18\n",
      "Tłumaczenie kandydujące:                          ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'miliatry', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
      "Tłumaczenie kandydujące bez powtórzeń:            ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'miliatry', 'always', 'obeys', 'commands', 'of', 'party']\n"
     ]
    }
   ],
   "source": [
    "# Ilość analizowanych n-gramów\n",
    "n = getNumberOfnGram(weights)\n",
    "print(\"{0:<50}\".format(\"Liczba rozpatrywanych n-gramów: \") + str(n))\n",
    "\n",
    "# Liczba referencji\n",
    "refNmb = len(references)\n",
    "print(\"{0:<50}\".format(\"Liczba tłumaczeń referencyjnych (wzorcowych): \") + str(refNmb))\n",
    "\n",
    "# Maksymalna długość wzorca\n",
    "refMaxLength = getMaxLengthOflist(references)\n",
    "print(\"{0:<50}\".format(\"Maksymalna długość wzorca: \") + str(refMaxLength))\n",
    "\n",
    "# Długość kandydata\n",
    "canLength = len(candidate)\n",
    "print(\"{0:<50}\".format(\"Maksymalna długość kandydata: \") + str(canLength))\n",
    "\n",
    "# Unikalne słowa kandydata \n",
    "print(\"{0:<50}\".format(\"Tłumaczenie kandydujące: \") + str(candidate))\n",
    "unique_candidate = makeUniqueList(candidate)\n",
    "print(\"{0:<50}\".format(\"Tłumaczenie kandydujące bez powtórzeń: \") + str(unique_candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d9812-d547-4572-8ccf-7f55c213e174",
   "metadata": {},
   "source": [
    "Wizuliacja i obliczenia BLEU: <br>\n",
    "<p style='text-align: justify;'>\n",
    "Obliczenie końcowej wartości metryki wymaga przeanalizowania podanej liczby n-gramów. Poszczególne, analizowane n-gramy utworzone z tłumaczenia ocenianego zawiera pierwsza kolumna tabeli oznaczona jako <b>n-GRAM</b> (warto zauważyć, że ilość unigramów może być mniejsza od bigramów, ponieważ ewaluowane tłumaczenie może zawierać powtarzające się słowa). Każdy n-gram jest wyszukiwany w tłumaczeniach referencyjnych. Kolumna <b>Ref</b> z numerem tłumaczenia zawiera informacje ile razy dany n-gram wystąpił w tym tłumaczeniu. Następująca kolumna <b>Max Ref</b> jest maksymalną liczbą wystąpień wśród wszystkich tłumaczeniach wzorcowych. Kolumna <b>Count</b> zawiera informacje o ilości wystąpień n-gramu w tłumaczeniu ocenianym, a kolumna <b>Clip Count</b> przechowuje minimalną wartość z Count i Max Ref. Najwiażniejsza kolumna <b>Contribution</b> wskazuje \"wkład\" danej grupy n-gramów do wyniku końcowego. Jest to stosunek sumy kolumny Clip Count do liczebności danej grupy n-gramów (uwzględniając powtórzenia). Wprowadzony mechanizm obliczania wartości w kolumnie Clip Count jest zmodyfikowaną precyzją n-gramu i pozwala na uniknięcie sytujacji, w której tłumaczenie otrzymuje nienaturalnie dużą wartość metryki poprzez podanie w tłumaczeniu kandydującym powtarzającego się słowa z tłumaczenia referencyjnego. \n",
    "<br>\n",
    "Przykładowo dla tłumaczeń wzorcowych: \"the cat is on the mat\" i \"there is a cat on the mat\" oraz tłumaczenia ocenianego \"the the the the the the the\" wkład unigramu \"the\" wynosi 2/7 zamiast 7/7.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8098dd8e-e140-47a5-b4a0-8b02a2620051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-GRAM\t\t\t\t\t\t\tRef1\tRef2\tRef3\tMax Ref Count\tClip Count\tContribution\n",
      "1)  It                  \t\t\t\t1\t1\t1\t1\t1\t1\n",
      "2)  is                  \t\t\t\t1\t1\t1\t1\t1\t1\n",
      "3)  a                   \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "4)  guide               \t\t\t\t1\t1\t1\t1\t1\t1\n",
      "5)  to                  \t\t\t\t1\t0\t1\t1\t1\t1\n",
      "6)  action              \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "7)  which               \t\t\t\t0\t1\t0\t1\t1\t1\n",
      "8)  ensures             \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "9)  that                \t\t\t\t2\t0\t0\t2\t1\t1\n",
      "10) the                 \t\t\t\t1\t4\t4\t4\t3\t3\n",
      "11) miliatry            \t\t\t\t1\t1\t0\t1\t1\t1\n",
      "12) always              \t\t\t\t0\t1\t1\t1\t1\t1\n",
      "13) obeys               \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "14) commands            \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "15) of                  \t\t\t\t0\t1\t1\t1\t1\t1\n",
      "16) party               \t\t\t\t0\t0\t1\t1\t1\t1\t\t16\\18\n",
      "\n",
      "1)  It is               \t\t\t\t1\t1\t1\t1\t1\t1\n",
      "2)  is a                \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "3)  a guide             \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "4)  guide to            \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "5)  to action           \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "6)  action which        \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "7)  which ensures       \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "8)  ensures that        \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "9)  that the            \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "10) the miliatry        \t\t\t\t1\t1\t0\t1\t1\t1\n",
      "11) miliatry always     \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "12) always obeys        \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "13) obeys the           \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "14) the commands        \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "15) commands of         \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "16) of the              \t\t\t\t0\t1\t1\t1\t1\t1\n",
      "17) the party           \t\t\t\t0\t0\t1\t1\t1\t1\t\t8\\17\n",
      "\n",
      "1)  It is a             \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "2)  is a guide          \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "3)  a guide to          \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "4)  guide to action     \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "5)  to action which     \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "6)  action which ensures\t\t\t\t0\t0\t0\t0\t1\t0\n",
      "7)  which ensures that  \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "8)  ensures that the    \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "9)  that the miliatry   \t\t\t\t1\t0\t0\t1\t1\t1\n",
      "10) the miliatry always \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "11) miliatry always obeys\t\t\t\t0\t0\t0\t0\t1\t0\n",
      "12) always obeys the    \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "13) obeys the commands  \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "14) the commands of     \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "15) commands of the     \t\t\t\t0\t0\t0\t0\t1\t0\n",
      "16) of the party        \t\t\t\t0\t0\t1\t1\t1\t1\t\t4\\16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printTable(n, refNmb, candidate, references, canLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177ba72-9df0-4266-a209-0d020ee21473",
   "metadata": {},
   "source": [
    "Obliczenie kary za różnice długości: <br>\n",
    "<p style='text-align: justify;'>\n",
    "Metryka BLEU uwzględnia również różnice w długościach pomiędzy tłumaczeniem ocenianym, a referencyjnymi. Jest to przeprowadzane poprzez mnożenie metryki przez wartość <b>BP</b> (z ang. Brevity Penalty) liczoną zgodnie ze wzorem: <br>\n",
    "</p>\n",
    "Dla c > r lub c = r:\n",
    "$$\n",
    "  BP = 1 \n",
    "$$\n",
    "Dla c < r:\n",
    "$$\n",
    "  BP = e^{1-\\frac{r}{c}}\n",
    "$$\n",
    "$r$ - liczba słów w wzorcowym tłumaczeniu<br>\n",
    "$c$ - liczba słów w kandydowanym tłumaczeniu<br>\n",
    "Otrzymana kara jest z przedziału 0 i 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e1a260-712b-475a-ae66-ca7c351738f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maksymalna długość tłumaczenia wzorcowego: 18\n",
      "Długość tłumaczenia kandydującego: 18\n",
      "BP: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Kara stratności:\n",
    "refLength = refMaxLength\n",
    "print(\"\\nMaksymalna długość tłumaczenia wzorcowego: \" + str(refLength))\n",
    "canLength = len(candidate)\n",
    "print(\"Długość tłumaczenia kandydującego: \" + str(canLength))\n",
    "bp = lengthPenalty(refLength, canLength)\n",
    "print(\"BP: \" + str(bp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe736b-392a-49fc-9f53-982514e0992f",
   "metadata": {},
   "source": [
    "Ostateczy wynik: <br>\n",
    "Końcowa wartość metryki jest liczona zgodnie ze wzorem:\n",
    "$$\n",
    "  BLUE = BP * exp \\Biggl ( \\sum \\limits_{n=1} ^{N} w_{n} log (p_{n}) \\Biggr )\n",
    "$$\n",
    "$N$ - liczba rozważanym n-gramów (otrzymana na podstawie zadeklarowanych wag w liście \"weights\")<br>\n",
    "$w_{n}$ - waga danego n-gramu (podana w liście \"weights\")<br>\n",
    "$p_{n}$ - stosunek wystąpień danego n-gramu do wszystkich n-gramów (wartość odczytana z kolumny \"Contribution\" dla danego n-gramu)<br>\n",
    "Końcowa wartość metryki powinna być z przedziału 0 i 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2004426f-b569-4fa8-87c2-8ad62040de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLUE [3] = 1.000 exp(  ln( 16/18 ) + ln( 8/17 ) + ln( 4/16 ) ) = 0.5686658363061537\n"
     ]
    }
   ],
   "source": [
    "calculateResult(bp, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95ac13-2e90-43b3-a24e-682814467437",
   "metadata": {},
   "source": [
    "Obliczenie wzorcowego BLEU: <br>\n",
    "Wartość metryki BLEU otrzymana przez gotowy pakiet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e40bd9c-a6db-4803-9e7d-ea224b9335df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wzorcowe BLEU Score:  0.5686658363061537\n"
     ]
    }
   ],
   "source": [
    "print(\"Wzorcowe BLEU Score: \", bleu.sentence_bleu(references, candidate, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e3b8a-9553-4d35-b23b-01b0b9b826ea",
   "metadata": {},
   "source": [
    "<b>Problemy metryki BLEU</b> <br>\n",
    "<a class=\"anchor\" id =\"BLEU_P\"></a>\n",
    "Wiele badań potwierdziło korelację wartości metryki BLEU oceną ludzką. Zauważa się jednak pewne problemy tej metryki:\n",
    "* Punkty dla słów posiadają taką samą wagę, więc zdania niekompletne nie są w żaden sposób niżej oceniane\n",
    "* Synonimy i parafrazy są brane pod uwagę wyłącznie wtedy, gdy występują w zbiorze tłumaczeń referencyjnych\n",
    "* Tłumaczenia otrzymujące podobne wartości metryki mogą uzyskać skrajne wartości w ocenie ludzkiej ze wględu na możliwości oszukania metryki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc74cfd-9c7c-4126-a8bf-3383c31b2ee9",
   "metadata": {},
   "source": [
    "### METEOR\n",
    "<a id =\"METEOR_O\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012cdad-05c4-4fdf-a185-5523e0fd63b5",
   "metadata": {},
   "source": [
    "**<font color='red'>M</font>etric for <font color='red'>E</font>valuation of <font color='red'>T</font>ranslation with <font color='red'>E</font>xplicit <font color='red'>OR</font>dering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bddbba-a9be-4ee5-873e-5d566fd4aedb",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "METEOR to metryka używana do oceny tłumaczenia maszynowego. Oparta jest na średniej harmonicznej n-gramów precyzji i pokrycia z przyznaniem większej wagi dla pokrycia. Cechą charakterystyczną tej metryki jest dopasowywanie synonimów - akceptowanie wyrazów o podobnym znaczeniu. Powodem powstania tej metryki jest próba wyeliminowania błędów pojawiających się w metryce BLEU. Główna różnicą pomiędzy tymi metrykami jest poziom szukania korelacji. BLUE skupia się na poziomie całego korpusu, natomiast METEOR na poziomie zdań i segmentów.\n",
    "<\\p>\n",
    "<br><br>\n",
    "<b>Działanie: </b> <br>\n",
    "<a id =\"METEOR_D\"></a>\n",
    "<p style='text-align: justify;'>\n",
    "METEOR w celu oszacowania jakości tłumaczenia maszynowego porównuje je z jednym/dwoma tłumaczeniami wzorcowymi. Wynik ewaluacji jest obliczany osobno dla każdego zdania w taki sposób, że każde zdanie z tłumaczenia maszynowego jest porównywane ze zdaniem z tłumaczenia wzorcowego, a następnie do dalszej analizy brana jest pod uwagę lepsza ocena. \n",
    "<\\p>\n",
    "Metryka METEOR posługuje się dwoma etapami:\n",
    "\n",
    "* Etap pierwszy: <br> METEOR ALIGNER - tworzenie odwzorowania pomiędzymi tłumaczeniami\n",
    "* Etap drugi: <br> METEOR SCORER - obliczanie końcowego wyniku\n",
    "\n",
    "    \n",
    "Etap pierwszy:<br>\n",
    "Etap pierwszy dzieli się na dwa zasadniczne kroki: zidentyfikowanie wszystkich odwzorowań pomiędzy tłumaczeniem ocenianym a wzorcowym i wyselekcjonowanie najlepszego odwzorowania. <br>\n",
    "\n",
    "Identyfikacja odwzorowań: <br>\n",
    "Odwzorowanień w metryce METEOR jest lista słów z tłumaczenia wzorcowego, które w jakiś spoób odpowiadają słowu z tłumaczenia maszynowego. Proces dopasowywania jest realizowany poprzez cztery moduły:\n",
    "* exact - dopasowanie słów identycznych\n",
    "* stem - dopasowanie słów o identycznym rdzeniu\n",
    "* synonym - dopasowanie słów bliskoznacznych według bazy WorldNet\n",
    "* paraphrase - dopasowanie fraz wymienionych jako parafrazy w tabeli parafrazowej\n",
    "\n",
    "Przykład: <br>\n",
    "Tłumaczenie maszynowe (T): Kot siedzi na ganku na złość <br>\n",
    "Tłumaczenie wzorcowe (T): Kotek na przekór siedział na płocie <br>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"METEOR_schemat.png\"/>\n",
    "</p>\n",
    "\n",
    "Wyselekcjonowanie najlepszego odwzorowania:\n",
    "Krok ten polega na znalezieniu największego podzbioru dopasowań wśród dopasowań znalezionych w pierwszym etapie i spełniających kryteria:\n",
    "* Każde słowo może należeć tylko do jednego dopasowania\n",
    "* Dopasowywana jest możliwie największa liczba słów w obu tłumaczeniach\n",
    "* W wyniku wybranych dopasowań występuje możliwie najmniejsza liczba fraz przylegających do siebie i występujących w tej samej kolejności w obu tłumaczeniach\n",
    "* Pomiędzy pozycjami startowymi dopasowań wystąpi jak najmniejsza suma odstępów - faworyzowanie dopasowań na podobnych pozycjach w obu tłumaczeniach\n",
    "\n",
    "Najlepsze dopasowanie:\n",
    "<p align=\"center\">\n",
    "  <img src=\"METEOR_schemat2.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd4250-886e-4e36-b826-8fa274c13c71",
   "metadata": {},
   "source": [
    "Etap drugi:<br>\n",
    "Celem etapu drugiego jest obliczenie wyniku końcowego metryki na podstawie wyniku pierwszego etapu. Końcowy wynik oparty jest na kilku wartościach:\n",
    "* Precyzja<br>\n",
    "Stosunek dopasowań wyrazów w ocenianym tłumaczeniu do wszystkich wyrazów tłumaczenia.\n",
    "$$\n",
    "  P = \\frac{\\sum \\limits_{i=1} ^{n} w_{i} * m_{i}(t)}{|t|}\n",
    "$$\n",
    "$n$ - liczba modułów biorących udział w dopasowaniu<br>\n",
    "$w_{i}$ - waga i-tego modułu<br>\n",
    "$m_{i}(t)$ - liczb wyrazów dopasowanych przez i-ty moduł<br>\n",
    "$|t|$ - liczba wszystkich wyrazów w tłumaczeniu<br>\n",
    "\n",
    "Przykład:\n",
    "$$\n",
    "  P = \\frac{w_{exact} * m_{exact}(t) + w_{stem} * m_{stem}(t) + w_{synonym} * m_{synonym}(t) + w_{paraphrase} * m_{paraphrase}(t) }{6}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "  P = \\frac{w_{exact} * 1 + w_{stem} * 1 + w_{synonym} * 1 + w_{paraphrase} * 2 }{6}\n",
    "$$\n",
    "* Pokrycie <br>\n",
    "Stosunek wyrazów w tłumaczeniu wzorcowym, które zostały dopasowane w tłumaczeniu maszynowym do wszystkich wyrazów tłumaczenia wzorcowego.\n",
    "$$\n",
    "  R = \\frac{\\sum \\limits_{i=1} ^{n} w_{i} * m_{i}(r)}{|r|}\n",
    "$$\n",
    "$m_{i}(r)$ - liczba wyrazów dopasowanych w tłumaczeniu referencyjnym<br>\n",
    "$|r|$ - liczba wyrazów tłumaczenia wzorcowego <br>\n",
    "\n",
    "Przykład:\n",
    "$$\n",
    "  P = \\frac{w_{exact} * m_{exact}(r) + w_{stem} * m_{stem}(r) + w_{synonym} * m_{synonym}(r) + w_{paraphrase} * m_{paraphrase}(r) }{6}\n",
    "$$\n",
    "$$\n",
    "  P = \\frac{w_{exact} * 1 + w_{stem} * 1 + w_{synonym} * 1 + w_{paraphrase} * 2 }{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f4f57-3a14-49db-b963-f1f21c2ade4e",
   "metadata": {},
   "source": [
    "### ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41d677-6e05-428c-a472-c539945498b9",
   "metadata": {},
   "source": [
    "**<font color='red'>R</font>ecall-<font color='red'>O</font>riented <font color='red'>U</font>nderstudy for <font color='red'>G</font>isting <font color='red'>E</font>valuation**\n",
    "<a id =\"ROGUE_O\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055aa707-fbee-44ac-9300-3696109d8d69",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "ROUGE to zbiór metryk używanych do oceny automatycznego streszczania plików i tłumaczenia maszynowego. Jest to bardzo prosta teoretycznie metryka posługująca się podczas swojego działania miarami: \"Recall\", \"Precision\" i \"F1 Score\". \n",
    "</p>\n",
    "\n",
    "Niemniej jednak, istnieje kilka różnych odmian metryki ROUGE:\n",
    "\n",
    "* ROUGE-N: Analizuje wystąpujące n-gramy pomiędzy danymi referencyjnymi i ocenianym modelem\n",
    "* ROUGE-L: Posługuje się najdłuższym wspólnym podciągiem pomiędzy danymi referencyjnymi i wzorcem\n",
    "* ROUGE-W: Posługuje się najdłuższym wspólnym podciągiem i wagami.\n",
    "\n",
    "Do najważniejszych z nich należy ROUGE-N i ROUGE-L."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3ec72-e2bf-4b12-b1b1-9da00b241d46",
   "metadata": {},
   "source": [
    "<b> ROUGE-N </b> </br>\n",
    "<a id =\"ROUGE_N\"></a>\n",
    "<p style='text-align: justify;'>\n",
    "Jest to jedna z podstawowych metryk zbioru metryk ROUGE. W swoim działaniu opiera się na mierzeniu licbzy pasujących n-gramu pomiedzy modelem (kandydatem), a danymi referencyjnymi (wzorcem). ROUGE-N może być rozpatrywany dla różnej, naturalnej wartości N. Jednakże, ROUGE-1, ROUGE-2 i ROUGE-3 to najczęściej spotykane szczególne przypadki metryki ROUGE-N. W praktyce oznacza to metryki ROUGE-N zorientowane kolejno na unigramach, bigramach i trigramach.\n",
    "</p>\n",
    "<b> Działanie: </b> </br>\n",
    "<a id =\"ROUGE_N_D\"></a>\n",
    "Metryka ROUGE-N posługuje się podczas działania miarami \"Recall\", \"Precison\" i \"F1 Score\" definiując ich wartości następująco:\n",
    "\n",
    "<h5>Recall</h5>\n",
    "<p style='text-align: justify;'>\n",
    "Miara Recall to stosunek liczby wspólnych n-gramów występujących pomiędzy wzorcem i kandydatem do liczby wszystkich n-gramów we wzorcu. Miara ta jest najczęściej liczona w oparciu o różne wartości naturalne n. \n",
    "</p>\n",
    "Przykładowo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f82b33aa-2065-4b10-8631-092408d9b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozpatrywany N-gram\n",
    "ngram = 1\n",
    "\n",
    "# Tłumaczenie referencyjne\n",
    "reference = 'the fox jumps'\n",
    "reference = reference.split()\n",
    "\n",
    "# Tłumaczenie kandydujące\n",
    "candidate = 'the hello a cat dog fox jumps'\n",
    "candidate = candidate.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d13004-224b-4608-b541-eb2c3361f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROUGE_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f380fdb9-ad02-49a6-9c53-2dc684f39d04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1)  the                 \t\t\t\t1\n",
      "2)  fox                 \t\t\t\t1\n",
      "3)  jumps               \t\t\t\t1\n",
      "Miara recall:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Miara recall: \", calculateRecall(ngram, reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d83f0-d961-4e45-bd7e-08ebda620769",
   "metadata": {},
   "source": [
    "<h5>Precision</h5>\n",
    "<p style='text-align: justify;'>\n",
    "Miare Precision jest miarą pozbawioną poważnej wady miary Recall, której wartość można zaburzyć poprzez wstawienie w miejsce modelu całego zbioru n-gramów. Takie podejście zagwarantuje, że każdy n-gram zostanie odnaleziony, a miara Recall zawsze będzie wynosić 1. W celu naprawy tego problemu Precision jest stosunekiem liczby wspólnych n-gramów występujących pomiędzy wzorcem i kandydatem do liczby n-gramów w kandydacie. Taka zmiana gwarantuje, że próba oszukania miary poprzez wstawienie wszystkich możliwości do badanego modelu spowoduje bardzo niskie wartości miary.\n",
    "</p>\n",
    "Przykładowo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2d2eda38-908f-4e38-b7a2-a40bb158d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyrażenia regularne do wyszukiwania słów w tekście:\n",
    "import re\n",
    "\n",
    "# Rozpatrywany N-gram\n",
    "ngram = 1\n",
    "\n",
    "# Tłumaczenie referencyjne\n",
    "reference = 'the fox jumps'.split()\n",
    "\n",
    "# Tłumaczenie kandydujące\n",
    "candidate = 'the hello a cat dog fox jumps'\n",
    "candidate = candidate.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "afbf3993-6e1e-40aa-a531-4ff5d7f21265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definiuje liczbę rozpatrywanych n-gramów zależnie od przypisanych wag\n",
    "def getNumberOfnGram(weights):\n",
    "    n = len(weights)\n",
    "    result = 0\n",
    "    for i in range(1, n):\n",
    "        if( weights[i - 1] == 0 ):\n",
    "            break\n",
    "        else:\n",
    "            result = i\n",
    "    return result + 1\n",
    "\n",
    "# Zwraca liczbę wystąpień pattern w text - wyrażenia regularne\n",
    "def getNumberOfOccurance(pattern, text):\n",
    "    # \"pattern\"\n",
    "    number = re.findall(\"^\" + pattern + \"$\", text)\n",
    "    # \"pattern \"\n",
    "    number = number + re.findall(\"^\" + pattern + \" \", text)\n",
    "    # \" pattern \"\n",
    "    number = number + re.findall(\" \" + pattern + \" \", text)\n",
    "    # \" pattern\"\n",
    "    number = number + re.findall(\" \" + pattern + \"$\", text)\n",
    "    if(len(number) > 0):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Tworzy z inputList listę bez duplikatów\n",
    "def makeUniqueList(inputList):\n",
    "    unique_list = []\n",
    "    for element in inputList:\n",
    "        if element not in unique_list:\n",
    "            unique_list.append(element)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2b5e83ef-99aa-4df9-898d-db0c2b2a5324",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1)  the                 \t\t\t\t1\n",
      "2)  fox                 \t\t\t\t1\n",
      "3)  jumps               \t\t\t\t1\n",
      "Miara recall:  0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# Zmienne pomocnicze\n",
    "unique_reference = makeUniqueList(reference)\n",
    "tabs = '\\t\\t\\t\\t'\n",
    "sumOfGram = 0\n",
    "\n",
    "# Liczba n-gramów:\n",
    "if(ngram > 1):\n",
    "    unique_reference = reference\n",
    "nmbOfGrams = len(unique_reference) - ngram + 1\n",
    "counter = 1\n",
    "    \n",
    "# Pętla po liczbie n-gramów zależna od rozpatrywanego n-gramu\n",
    "for idOfPattern in range(0, nmbOfGrams):\n",
    "        \n",
    "    gram = unique_reference[idOfPattern]\n",
    "        \n",
    "    # Pętla po liczbie słów do dodania aby otrzymać n-gram\n",
    "    for idToAdd in range(1, ngram):\n",
    "        gram = gram + ' ' + unique_reference[idOfPattern + idToAdd]\n",
    "        \n",
    "    #Wypisanie początku wiersza tabeli\n",
    "    print(\"{0:<4}\".format(str(counter)+ \")\"), end='')\n",
    "    print(\"{0:<20}\".format(gram), end='')\n",
    "    print(tabs, end='')\n",
    "        \n",
    "    #Zmienne dla konkretnego n-gramu:\n",
    "    counter = counter + 1\n",
    "    candidateWithSpace = ' '.join(map(str, candidate))\n",
    "        \n",
    "    # Sprawdzenie wystąpienia konkretnego n-gramu w referencji\n",
    "    gramOccurrances = getNumberOfOccurance(gram, candidateWithSpace)\n",
    "    sumOfGram = sumOfGram + gramOccurrances\n",
    "    print(str(gramOccurrances))\n",
    "        \n",
    "precision = sumOfGram/(len(makeUniqueList(candidate)) - ngram + 1)\n",
    "\n",
    "print(\"Miara recall: \", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa1432-cd14-4f50-99d0-1c77311ed277",
   "metadata": {},
   "source": [
    "<h5>F1 Score</h5>\n",
    "<p style='text-align: justify;'>\n",
    "Miara F1 Score jest połączenie miary \"Recall\" i \"Precision\" zgodnie ze wzorem:\n",
    "\n",
    "$$\n",
    "F1_{Score} = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$\n",
    "Wartość miary F1 daje nam wiarygodną informacje o jakości naszego modelu, która jest uzależniona nie tylko od skuteczności naszego modelu, która można zawyżyć używając wszystkich przypadków (recall), ale uwzględnia również nieistone n-gramy.\n",
    "</p>\n",
    "Przykładowo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "70ffa20d-9ba9-4765-b1c5-dd490b5c44d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miara F1:  0.6\n"
     ]
    }
   ],
   "source": [
    "F1 = 2 * (recall * precision)/(precision + recall)\n",
    "print(\"Miara F1: \", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d171a-2e04-447f-b769-6ffd728f224d",
   "metadata": {},
   "source": [
    "<b> ROUGE-L </b> </br>\n",
    "<a id =\"ROUGE_L\"></a>\n",
    "<p style='text-align: justify;'>\n",
    "Jest to również jedna z bazowych metryk ROUGE. Jej działanie opiera się na najdłuższym wspólnym podciągu znalezionym pomiedzy modelem i referencją. Najdłuższy wspólny podciąg jest rozumiany jako najdłuższy podciąg znaków, który występuje w tej samej kolejności w dwóch porównywalnych łańcuchach znaków. Elementy podciągów nie muszą przy tym leżeć obok siebie.\n",
    "</p>\n",
    "<b> Działanie: </b> </br>\n",
    "<a id =\"ROUGE_L_D\"></a>\n",
    "<p style='text-align: justify;'>\n",
    "ROUGE-L posługuje się najdłuższym wspólnym podciągiem obu wyrazów, ale w operując na poziomie całych słów. Długość NWP oznacza ilość pełnych słów w danym znalezionym podciągu, a nie, jak standardowo, ilość znaków w podciągu.\n",
    "</p>\n",
    "<h5>Najdłuższy wspólny podciąg na poziomie słów:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae853c8-9aa4-4521-adba-a0913e43cc70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wyrażenia regularne do wyszukiwania słów w tekście:\n",
    "import re\n",
    "\n",
    "# Rozpatrywany N-gram\n",
    "ngram = 1\n",
    "\n",
    "# Tłumaczenie referencyjne\n",
    "reference = 'the fox jumps'.split()\n",
    "\n",
    "# Tłumaczenie kandydujące\n",
    "candidate = 'the hello a cat dog fox jumps'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949e7a0e-ddfb-4dbb-96ef-bf7f3be31ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funkcja zwracająca długość najdłuższego wspólnego podłańcucha\n",
    "def longestCommonSubsequence(text1 , text2):\n",
    "    m = len(text1)\n",
    "    n = len(text2)\n",
    "    matrix = [[0]*(n+1) for i in range(m+1)] \n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if i==0 or j==0:\n",
    "                matrix[i][j] = 0\n",
    "            elif text1[i-1] == text2[j-1]:\n",
    "                matrix[i][j] = 1 + matrix[i-1][j-1]\n",
    "            else:\n",
    "                matrix[i][j] = max(matrix[i-1][j] , matrix[i][j-1])\n",
    "    return matrix[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a160c89e-fbca-4ec3-80e0-b4170fee4ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Długość najdłuższego wspólnego podłańcucha:  3\n"
     ]
    }
   ],
   "source": [
    "length = longestCommonSubsequence(candidate, reference)\n",
    "print(\"Długość najdłuższego wspólnego podłańcucha: \", length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159132cb-62b9-44c0-977d-b49d64d7c1e4",
   "metadata": {},
   "source": [
    "Kolejnym krokiem po znalezieniu NWP jest obliczenie wartości metryki ROUGE-L zgodnie ze wzorami:\n",
    "$$\n",
    "Recall = \\frac{l}{n}\n",
    "$$\n",
    "$l$ - długość NWP <br>\n",
    "$n$ - ilość unigramów w referencji <br>\n",
    "\n",
    "$$\n",
    "Precision = \\frac{l}{m}\n",
    "$$\n",
    "$l$ - długość NWP <br>\n",
    "$m$ - ilość unigramów w kandydacie <br>\n",
    "\n",
    "$$\n",
    "F1_{Score} = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d52e71a9-1eaa-4eaf-bced-622d20a8ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 1.0\n",
      "Precision: 0.42857142857142855\n",
      "F1 Score: 0.6\n"
     ]
    }
   ],
   "source": [
    "recall = length/len(reference)\n",
    "print(\"Recall: \" + str(recall))\n",
    "precision = length/len(candidate)\n",
    "print(\"Precision: \" + str(precision))\n",
    "f1 = 2 * (recall * precision)/(precision + recall)\n",
    "print(\"F1 Score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0df95-1cca-45fc-9ada-ee8dc58d6262",
   "metadata": {},
   "source": [
    "<b>Python Rouge Library</b>\n",
    "<br>\n",
    "\"Python rouge library\" jest dodatkową biblioteką umożliwiająca liczenie wartości metryki ROUGE. Domyślnie wynik jest podawamy dla ROUGE-1, ROUGE-2 i ROUGE-L z podziałem na wartość:\n",
    "* \"r\" - Recall, \n",
    "* \"p\" - Precision, \n",
    "* \"f\" - \"F1 Score\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "618dab47-b6dc-4f86-b717-917e5049ddc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 1.0, 'p': 0.42857142857142855, 'f': 0.5999999958},\n",
       "  'rouge-2': {'r': 0.5, 'p': 0.16666666666666666, 'f': 0.24999999625000005},\n",
       "  'rouge-l': {'r': 1.0, 'p': 0.42857142857142855, 'f': 0.5999999958}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "candidate = \"the hello a cat dog fox jumps\"\n",
    "\n",
    "reference = \"the fox jumps\"\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge.get_scores(candidate, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b71d2-404c-4e6e-ac93-e90e1919b939",
   "metadata": {},
   "source": [
    "### WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361a0e3-7b62-4378-bc57-687997f1fa70",
   "metadata": {},
   "source": [
    "**<font color='red'>W</font>ord <font color='red'>M</font>over's <font color='red'>D</font>instance**\n",
    "<a id =\"WMD_O\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf04010-1052-4a84-b5a7-f4ad9a31b575",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "WMD to relatywnie nowe narzędzie funkcjonujące w oparciu o uczenie maszynowe używane jako metryka jakości podobieństwa dwóch dokumnetów. Najważniejszą cechą metryki WMD jest możliwość oceny dwóch tekstów o podobnym znaczeniu, ale używających różnych słów do wyrażenia tej samej idei. \n",
    "</p>\n",
    "\n",
    "<b> Działanie: </b> </br>\n",
    "<a id =\"WMD_D\"></a>\n",
    "Celem WMD jest zmierzenie odległości semantycznej dwóch tekstów, która jest szacowana z uwzględnieniem możliwości występowania synonimów. Typowym przykładem przy omawianiu metryki WMD są zdania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94eabd6e-beed-4939-9abb-cf99fdd75bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zdanie 1\n",
    "reference = 'Obama speaks to the media in Illinois'.split()\n",
    "\n",
    "# Zdanie 2\n",
    "candidate = 'The president greets the press in Chicago'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34288cc8-78e3-4646-8694-dd3f2c1ecc64",
   "metadata": {},
   "source": [
    "Oba zdania wyrażają bardzo podobną myśl przy użyciu całkowicie odmiennych słów. Spodziewana wartość metryki w takiej sytuacji jest dość wysoka.\n",
    "<p align=\"center\">\n",
    "  <img src=\"WMD_schemat.png\"/>\n",
    "</p>\n",
    "Powyższe zdjęcie przedstawia wizualizacje tych dwóch zdań stosująć wektorową reprezentację dokumentów \"word2vec\". Zasada jej działania polega na minimalizacji dystanu pomiędzy słowami, które często występują w swoim otoczeniu. Przykładowo słowo \"kwiatek\" i \"łąka\" powinny być bliżej siebie niż \"kwiatek\" i \"pustynia\".\n",
    "<br>\n",
    "\n",
    "Analiza omawianego przykładu zaczyna się od usunięcia tzw. \"przerywników\", które niepotrzebnie zwiększają złożoność algorytmu i prowadzą do błędów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e1cd4b6-f54f-4cf5-9452-1bfedd2d409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zdanie 1 po usunięciu przerywników:  president greets press chicago\n",
      "Zdanie 2 po usunięciu przerywników:  obama speaks media illinois\n"
     ]
    }
   ],
   "source": [
    "# Usunięcie wielkich liter i podział na listy słów\n",
    "reference = [w.lower() for w in reference]\n",
    "candidate = [w.lower() for w in candidate]\n",
    "\n",
    "# Pobranie zbioru \"przerywników\" dla języka angielskiego\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# Usunięcie przerywników\n",
    "reference = [w for w in reference if w not in stopWords]\n",
    "candidate = [w for w in candidate if w not in stopWords]\n",
    "\n",
    "print(\"Zdanie 1 po usunięciu przerywników: \", ' '.join(map(str, candidate)))\n",
    "print(\"Zdanie 2 po usunięciu przerywników: \", ' '.join(map(str, reference)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49872e98-766c-433b-ac7e-7a2d3271980b",
   "metadata": {},
   "source": [
    "Następnie dla skróconych zdań jest liczony koszt zmiany każdego słowa pierwszego zdania na każdego słowo drugiego zdania i wybierany jest minimalny koszt. Jest to krok konieczny, ponieważ algorytm nie wie, że przykładowo najmniejszym kosztem będzie charakteryzować się zamiana \"obama\" na \"president\", ponieważ oba słowa często występują w swoim otoczeniu. Wynikiem metryki jest najmniejszy znaleziony dystans pomiędzy zdaniami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ac136-b7f3-42e8-be39-cc7d4325a38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc2f53-d16d-476b-8ca1-5ddc5abb8583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da0dea50-0fd3-40e0-a529-294161082c37",
   "metadata": {},
   "source": [
    "### CIDEr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4611113f-1fd7-498a-bb3b-9b8e29c7b8c8",
   "metadata": {},
   "source": [
    "**<font color='red'>C</font>onsensus-based <font color='red'>I</font>mage <font color='red'>D</font>escription <font color='red'>E</font>valuation**\n",
    "\n",
    "<a id =\"WMD_O\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6ab3f-636c-4f6c-9911-6524539ac7ef",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Metryka CIDEr mierzy zgodność pomiędzy opisami dwóch obrazów w oparciu o metodę TF-IDF (ważenie częstością termów -metoda pozwalająca na obliczenie wagi słów w oparciu o liczbę wystąpień) liczoną dla każdego n-gramu. Liczba wystąpień n-gramu $w_k$ w zdaniu referencyjnym $s_{ij}$ jest oznaczona przez $h_{k}(s_{ij})$ lub $h_{k}(c_{i})$ dla zdania ocenianego (kandydującego) $c_i$. Metryka CIDEr przelicza wartość wagi TF-IDF $g_{k}(s_{ij})$ dla każdego n-gram $w_{k}$ zgodnie ze wzorem:\n",
    "</p>\n",
    "$$\n",
    "g_{k}(s_{ij}) = \\frac{h_{k}(s_{ij})}{\\sum \\limits_{w_l \\in \\Omega} h_{l}(s_{ij})} log \\Biggl( \\frac{|I|} {\\sum \\limits_{I_p \\in I} min(1, \\sum \\limits_{q} h_k(s_{ij}))} \\Biggr)\n",
    "$$\n",
    "gdzie: \n",
    "<br>\n",
    "$ \\Omega $ - słownictwo związane z każdym n-gramem <br>\n",
    "$ I $ - zbiór wszystkich obrazów w zbiorze danych\n",
    "<p style='text-align: justify;'>\n",
    "Pierwsza część odpowiada za mierzenie TF każdego n-gramu $w_k$, a druga część mierzy rzadkość $w_k$ używając IDF. Zgodnie z intuicją TF przypisuje większą wagę do n-gramów, które często wystąpują w zdaniach referencyjnych opisujących obraz, a jednocześnie IDF zmniejsza wage n-gramów, które często występują we wszystkich opisach. W konsekwencji IDF odpowiada za zapewnienie zachowania istotności wyrazów poprzez zaniżanie wartości częstych słów, które mają małe prawdopodobieństwo na przechowywania ważnych informacji o opisywanym obrazie. IDF jest obliczany jako logarytm liczby obrazów w zbiorze |I| przez liczbę obrazów, dla których $w_k$ występuje w dowolnym z jego zdań referencyjnych.\n",
    "\n",
    "Wartoś metryki CIDEr jest obliczona na podstawie wszystkich n-gramów o długości n używając średniego podobieństwa cosinusowego pomiędzy kandydatem i zdaniami referencyjnymi \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff8acf-a6b6-4314-a1ba-ab6962438672",
   "metadata": {},
   "source": [
    "BLEU:\n",
    "* https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1\n",
    "</br>\n",
    "\n",
    "BLEU i METEOR:\n",
    "* http://docplayer.pl/14218168-Ewaluacja-systemow-tlumaczenia-automatycznego.html\n",
    "</br>\n",
    "\n",
    "ROUGE:\n",
    "* https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460\n",
    "* https://en.wikipedia.org/wiki/Automatic_summarization\n",
    "* https://en.wikipedia.org/wiki/ROUGE_(metric)\n",
    "* https://pl.wikipedia.org/wiki/Najdłuższy_wspólny_podciąg\n",
    "\n",
    "WMD:\n",
    "* https://towardsdatascience.com/word-distance-between-word-embeddings-cc3e9cf1d632\n",
    "* https://ermlab.com/blog/technicznie/doc2vec-wektorowa-reprezentacja-dokumentow/\n",
    "* https://medium.com/@nihitextra/word-movers-distance-for-text-similarity-7492aeca71b0\n",
    "\n",
    "CIDEr:\n",
    "* https://arxiv.org/pdf/1504.00325.pdf\n",
    "* https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755c315-d803-4143-bf6a-584967d58740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
